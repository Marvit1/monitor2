print("üîç Debug: Starting imports...")
import time
print("üîç Debug: time imported")
import os
print("üîç Debug: os imported")
import sys
print("üîç Debug: sys imported")
import subprocess
print("üîç Debug: subprocess imported")
import requests
print("üîç Debug: requests imported")
import json
print("üîç Debug: json imported")
from datetime import datetime, timedelta
print("üîç Debug: datetime imported")
print("üîç Debug: All imports completed")

# Memory optimization imports
import gc
import psutil

def cleanup_memory():
    """Memory cleanup function"""
    try:
        # Force garbage collection
        gc.collect()
        
        # Get memory info
        process = psutil.Process()
        memory_info = process.memory_info()
        memory_mb = memory_info.rss / 1024 / 1024
        
        print(f"üßπ Memory cleanup: {memory_mb:.1f} MB")
        return memory_mb
    except Exception as e:
        print(f"‚ö†Ô∏è Memory cleanup error: {e}")
        return 0

class NewsMonitorAPI:
    def __init__(self, api_base_url):
        self.api_base_url = api_base_url.rstrip('/')
        self.session = requests.Session()
        self.session.headers.update({
            'Content-Type': 'application/json',
            'User-Agent': 'NewsMonitor/1.0'
        })
        
    def test_connection(self):
        """API-’´ ’Ø’°’∫’´ ’Ω’ø’∏÷Ç’£’∏÷Ç’¥"""
        try:
            print(f"üîó ’ç’ø’∏÷Ç’£’∏÷Ç’¥ ’•’∂÷Ñ API ’Ø’°’∫’®’ù {self.api_base_url}")
            
            # ’ì’∏÷Ä’±’∏÷Ç’¥ ’•’∂÷Ñ ’∞’´’¥’∂’°’Ø’°’∂ endpoint-’∂’•÷Ä’®
            test_endpoints = [
                f"{self.api_base_url}/",
                f"{self.api_base_url}/api/",
                f"{self.api_base_url}/health/",
                f"{self.api_base_url}/status/"
            ]
            
            for endpoint in test_endpoints:
                try:
                    response = self.session.get(endpoint, timeout=5)
                    if response.status_code in [200, 404]:  # 404-’® ’∂’∏÷Ç’µ’∂’∫’•’Ω OK ’ß, ’∂’∑’°’∂’°’Ø’∏÷Ç’¥ ’ß endpoint ’£’∏’µ’∏÷Ç’©’µ’∏÷Ç’∂ ’∏÷Ç’∂’´
                        print(f"‚úÖ API ’Ø’°’∫ ’∞’°’ª’∏’≤’ù {endpoint}")
                        return True
                except:
                    continue
            
            print(f"‚ö†Ô∏è API ’Ø’°’∫’´ ’≠’∂’§’´÷Ä’ù {self.api_base_url}")
            return False
            
        except Exception as e:
            print(f"‚ùå API ’Ø’°’∫’´ ’Ω’≠’°’¨’ù {e}")
            return False
    
    def cleanup_old_articles(self, days_to_keep):
        """’Ä’´’∂ ’∞’∏’§’æ’°’Æ’∂’•÷Ä’´ ’¥’°÷Ñ÷Ä’∏÷Ç’¥ API-’´ ’¥’´’ª’∏÷Å’∏’æ"""
        try:
            cleanup_date = (datetime.now() - timedelta(days=days_to_keep)).isoformat()
            print(f"üßπ ’ì’∏÷Ä’±’∏÷Ç’¥ ’•’∂÷Ñ ’¥’°÷Ñ÷Ä’•’¨ ’∞’∏’§’æ’°’Æ’∂’•÷Ä’® {cleanup_date} ’°’¥’Ω’°’©’æ’´÷Å ’°’º’°’ª...")
            
            # ’ì’∏÷Ä’±’∏÷Ç’¥ ’•’∂÷Ñ ’ø’°÷Ä’¢’•÷Ä endpoint-’∂’•÷Ä
            endpoints_to_try = [
                f"{self.api_base_url}/api/articles/cleanup/",
                f"{self.api_base_url}/api/cleanup/",
                f"{self.api_base_url}/api/articles/",
                f"{self.api_base_url}/cleanup/"
            ]
            
            for endpoint in endpoints_to_try:
                try:
                    print(f"üîó ’ì’∏÷Ä’±’∏÷Ç’¥ ’•’∂÷Ñ endpoint’ù {endpoint}")
                    response = self.session.delete(
                        endpoint,
                        params={'before_date': cleanup_date},
                        timeout=10
                    )
                    
                    if response.status_code == 200:
                        data = response.json()
                        deleted_count = data.get('deleted_count', 0)
                        print(f"‚úÖ ’Ä’°’ª’∏’≤’∏÷Ç’©’µ’°’¥’¢ ’¥’°÷Ñ÷Ä’æ’•÷Å {deleted_count} ’∞’∏’§’æ’°’Æ")
                        return deleted_count
                    elif response.status_code == 404:
                        print(f"‚ö†Ô∏è Endpoint ’π’´ ’£’ø’∂’æ’•’¨’ù {endpoint}")
                        continue
                    else:
                        print(f"‚ùå API cleanup error: {response.status_code} - {response.text}")
                        continue
                        
                except requests.exceptions.RequestException as e:
                    print(f"‚ö†Ô∏è Network error {endpoint}: {e}")
                    continue
            
            print("‚ùå ’à’π ’¥’´ endpoint ’π’°’∑’≠’°’ø’•÷Å")
            return 0
            
        except Exception as e:
            print(f"‚ùå API cleanup exception: {e}")
            return 0
    
    def get_keywords(self):
        """‘≤’°’∂’°’¨’´ ’¢’°’º’•÷Ä’´ ’Ω’ø’°÷Å’∏÷Ç’¥ API-’´÷Å"""
        try:
            print(f"üîç ’ì’∏÷Ä’±’∏÷Ç’¥ ’•’∂÷Ñ ’Ω’ø’°’∂’°’¨ ’¢’°’∂’°’¨’´ ’¢’°’º’•÷Ä...")
            
            # ’ì’∏÷Ä’±’∏÷Ç’¥ ’•’∂÷Ñ ’ø’°÷Ä’¢’•÷Ä endpoint-’∂’•÷Ä
            endpoints_to_try = [
                f"{self.api_base_url}/api/keywords/",
                f"{self.api_base_url}/api/keywords",
                f"{self.api_base_url}/keywords/",
                f"{self.api_base_url}/keywords"
            ]
            
            for endpoint in endpoints_to_try:
                try:
                    print(f"üîó ’ì’∏÷Ä’±’∏÷Ç’¥ ’•’∂÷Ñ endpoint’ù {endpoint}")
                    response = self.session.get(endpoint, timeout=10)
                    
                    if response.status_code == 200:
                        keywords = response.json()
                        print(f"‚úÖ ’ç’ø’°÷Å’æ’•÷Å {len(keywords)} ’¢’°’∂’°’¨’´ ’¢’°’º")
                        return keywords
                    elif response.status_code == 404:
                        print(f"‚ö†Ô∏è Endpoint ’π’´ ’£’ø’∂’æ’•’¨’ù {endpoint}")
                        continue
                    else:
                        print(f"‚ùå API keywords error: {response.status_code} - {response.text}")
                        continue
                        
                except requests.exceptions.RequestException as e:
                    print(f"‚ö†Ô∏è Network error {endpoint}: {e}")
                    continue
            
            print("‚ùå ’à’π ’¥’´ keywords endpoint ’π’°’∑’≠’°’ø’•÷Å")
            return []
            
        except Exception as e:
            print(f"‚ùå API keywords exception: {e}")
            return []

def run_scrapy_with_reactor_fix(spider_name, scrapy_project_path):
    """Run scrapy with reactor signal handling fix"""
    try:
        # Set environment variables to fix reactor issues
        env = dict(os.environ)
        env.update({
            'SCRAPY_SETTINGS_MODULE': 'news_scraper.settings',
            'PYTHONPATH': scrapy_project_path,
            # Disable signal handling that causes issues in containerized environments
            'TWISTED_DISABLE_SIGNAL_HANDLERS': '1',
            'TWISTED_NO_SIGNAL_HANDLERS': '1',
            # Suppress all warnings
            'PYTHONWARNINGS': 'ignore'
        })
        
        # Use simple scrapy crawl command with environment variables
        result = subprocess.run([
            sys.executable, '-m', 'scrapy', 'crawl', spider_name
        ], 
            cwd=scrapy_project_path,
            capture_output=True,
            text=True,
            timeout=120,  # 2 minutes per spider
            env=env
        )
        
        # Print full error details for debugging
        if result.returncode != 0:
            print(f"‚ùå Spider {spider_name} failed with return code: {result.returncode}")
            print(f"üìÑ STDOUT: {result.stdout}")
            print(f"‚ùå STDERR: {result.stderr}")
        
        # Memory cleanup after spider finishes
        memory_usage = cleanup_memory()
        print(f"üßπ Spider {spider_name} finished, memory usage: {memory_usage:.1f} MB")
        
        return result
        
    except Exception as reactor_error:
        print(f"‚ùå Scrapy crawl failed: {reactor_error}")
        # Return a mock result to prevent crashes
        from types import SimpleNamespace
        mock_result = SimpleNamespace()
        mock_result.returncode = 1
        mock_result.stdout = ""
        mock_result.stderr = f"Scrapy crawl failed: {reactor_error}"
        return mock_result

def get_spiders_list(scrapy_project_path):
    """Get list of available spiders by scanning spider files directly"""
    spiders = []
    
    # Method 1: Direct file scanning (most reliable)
    spiders_dir = os.path.join(scrapy_project_path, 'news_scraper', 'spiders')
    print(f"üîç ’ç’ø’∏÷Ç’£’∏÷Ç’¥ ’•’∂÷Ñ spiders ’∫’°’∂’°’Ø’®’ù {spiders_dir}")
    
    if os.path.exists(spiders_dir):
        try:
            for filename in os.listdir(spiders_dir):
                if filename.endswith('.py') and filename != '__init__.py':
                    spider_file = os.path.join(spiders_dir, filename)
                    print(f"üìÑ ‘≥’ø’∂’æ’°’Æ spider ÷Ü’°’µ’¨’ù {filename}")
                    
                    # Try to extract spider name from file
                    try:
                        with open(spider_file, 'r', encoding='utf-8') as f:
                            content = f.read()
                            # Look for name = 'spider_name' pattern
                            import re
                            name_matches = re.findall(r"name\s*=\s*['\"]([^'\"]+)['\"]", content)
                            if name_matches:
                                spider_name = name_matches[0]
                                spiders.append(spider_name)
                                print(f"‚úÖ ‘≥’ø’∂’æ’°’Æ spider’ù {spider_name}")
                            else:
                                # Fallback: use filename without .py
                                spider_name = filename[:-3]
                                if spider_name not in ['__init__', 'base']:
                                    spiders.append(spider_name)
                                    print(f"‚úÖ Fallback spider’ù {spider_name}")
                    except Exception as e:
                        print(f"‚ö†Ô∏è ’â’∞’°’ª’∏’≤’æ’•÷Å ’Ø’°÷Ä’§’°’¨ {filename}: {e}")
                        
        except Exception as e:
            print(f"‚ùå Spiders ’∫’°’∂’°’Ø’´ ’Ω’Ø’°’∂’°’æ’∏÷Ä’¥’°’∂ ’Ω’≠’°’¨: {e}")
    else:
        print(f"‚ùå Spiders ’∫’°’∂’°’Ø’® ’£’∏’µ’∏÷Ç’©’µ’∏÷Ç’∂ ’π’∏÷Ç’∂’´’ù {spiders_dir}")
    
    # Method 2: Try Scrapy's spider loader as fallback (only if Method 1 failed)
    if not spiders:
        print("üîÑ ’ì’∏÷Ä’±’∏÷Ç’¥ ’•’∂÷Ñ Scrapy-’´ spider loader...")
        try:
            env = dict(os.environ)
            env.update({
                'SCRAPY_SETTINGS_MODULE': 'news_scraper.settings',
                'PYTHONPATH': scrapy_project_path,
                # Use Python 3.11 for better compatibility
                'PYTHON_VERSION': '3.11',
                # Use stable Twisted version
                'TWISTED_VERSION': '22.10.0',
                # Force asyncio reactor before any imports
                'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
                'TWISTED_DISABLE_SIGNAL_HANDLERS': '1',
                'TWISTED_NO_SIGNAL_HANDLERS': '1',
                'PYTHONWARNINGS': 'ignore'
            })
            
            # Simpler approach without using deprecated modules
            result = subprocess.run([
                sys.executable, '-W', 'ignore', '-c', f'''
import warnings
warnings.filterwarnings("ignore")

import sys
import os
sys.path.insert(0, "{scrapy_project_path}")

try:
    import importlib.util
    import glob
    
    spider_files = glob.glob("{spiders_dir}/*.py")
    for spider_file in spider_files:
        if not spider_file.endswith("__init__.py"):
            with open(spider_file, 'r', encoding='utf-8') as f:
                content = f.read()
                if 'class' in content and 'Spider' in content:
                    import re
                    names = re.findall(r"name\\s*=\\s*['\\""]([^'\\"\"]+)['\\""]", content)
                    if names:
                        print(names[0])
                    else:
                        basename = os.path.basename(spider_file)[:-3]
                        if basename not in ['__init__', 'base']:
                            print(basename)
except Exception as subprocess_error:
    print(f"Error: {{subprocess_error}}", file=sys.stderr)
    sys.exit(1)
'''],
                cwd=scrapy_project_path,
                capture_output=True,
                text=True,
                timeout=30,
                env=env
            )
            
            if result.returncode == 0 and result.stdout.strip():
                fallback_spiders = [spider.strip() for spider in result.stdout.strip().split('\n') if spider.strip()]
                spiders.extend(fallback_spiders)
                print(f"‚úÖ Scrapy loader ’°÷Ä’§’µ’∏÷Ç’∂÷Ñ’ù {fallback_spiders}")
            else:
                print(f"‚ùå Scrapy loader ’±’°’≠’∏’≤’∏÷Ç’¥: {result.stderr}")
                
        except Exception as e:
            print(f"‚ùå Scrapy loader ’Ω’≠’°’¨: {e}")
    
    # Method 3: Hardcoded fallback for common spider names
    if not spiders:
        print("üîÑ ’ì’∏÷Ä’±’∏÷Ç’¥ ’•’∂÷Ñ ’Ω’ø’°’∂’§’°÷Ä’ø spider ’°’∂’∏÷Ç’∂’∂’•÷Ä...")
        common_spiders = ['asbarez', 'aravot', 'news_am', 'panorama', 'hetq']
        for spider_name in common_spiders:
            spider_file = os.path.join(spiders_dir, f'{spider_name}.py')
            if os.path.exists(spider_file):
                spiders.append(spider_name)
                print(f"‚úÖ ’ç’ø’°’∂’§’°÷Ä’ø spider ’£’ø’∂’æ’°’Æ’ù {spider_name}")
    
    return list(set(spiders))  # Remove duplicates

def check_project_structure(scrapy_project_path):
    """Check and debug project structure"""
    print(f"üîç ’ç’ø’∏÷Ç’£’∏÷Ç’¥ ’•’∂÷Ñ ’∂’°’≠’°’£’Æ’´ ’Ø’°’º’∏÷Ç÷Å’æ’°’Æ÷Ñ’®...")
    print(f"üìÅ Project path: {scrapy_project_path}")
    print(f"üìÅ Exists: {os.path.exists(scrapy_project_path)}")
    
    if os.path.exists(scrapy_project_path):
        print("üìÇ Project directory contents:")
        try:
            for item in os.listdir(scrapy_project_path):
                item_path = os.path.join(scrapy_project_path, item)
                if os.path.isdir(item_path):
                    print(f"  üìÅ {item}/")
                else:
                    print(f"  üìÑ {item}")
        except Exception as e:
            print(f"‚ùå Could not list project directory: {e}")
    
    # Check news_scraper directory
    news_scraper_path = os.path.join(scrapy_project_path, 'news_scraper')
    print(f"üìÅ news_scraper path: {news_scraper_path}")
    print(f"üìÅ Exists: {os.path.exists(news_scraper_path)}")
    
    if os.path.exists(news_scraper_path):
        print("üìÇ news_scraper directory contents:")
        try:
            for item in os.listdir(news_scraper_path):
                item_path = os.path.join(news_scraper_path, item)
                if os.path.isdir(item_path):
                    print(f"  üìÅ {item}/")
                else:
                    print(f"  üìÑ {item}")
        except Exception as e:
            print(f"‚ùå Could not list news_scraper directory: {e}")
    
    # Check spiders directory
    spiders_path = os.path.join(scrapy_project_path, 'news_scraper', 'spiders')
    print(f"üìÅ spiders path: {spiders_path}")
    print(f"üìÅ Exists: {os.path.exists(spiders_path)}")
    
    if os.path.exists(spiders_path):
        print("üìÇ spiders directory contents:")
        try:
            for item in os.listdir(spiders_path):
                item_path = os.path.join(spiders_path, item)
                if os.path.isdir(item_path):
                    print(f"  üìÅ {item}/")
                else:
                    print(f"  üìÑ {item}")
        except Exception as e:
            print(f"‚ùå Could not list spiders directory: {e}")

def main():
    print("üè¢ ‘Ω’à’í’Ñ‘≤ 1 - ’Ñ’•’Æ ’∂’µ’∏÷Ç’¶ ’Ω’°’µ’ø’•÷Ä’´ ’¥’∏’∂’´’ø’∏÷Ä’´’∂’£ (news_scraper_group1)")
    print("üîç Debug: main() function started")
    
    # Get settings from environment variables or use defaults
    interval_minutes = int(os.environ.get('MONITOR_INTERVAL_MINUTES', 2))
    days_to_keep = int(os.environ.get('DAYS_TO_KEEP_ARTICLES', 7))
    api_base_url = os.environ.get('API_BASE_URL', 'https://beackkayq.onrender.com')
    
    print(f"üåê API Base URL: {api_base_url}")
    print(f"üîç Debug: interval_minutes = {interval_minutes}")
    print(f"üîç Debug: days_to_keep = {days_to_keep}")
    
    print("üöÄ ‘Ω’à’í’Ñ‘≤ 1 - ’Ñ’•’Æ ’∂’µ’∏÷Ç’¶ ’Ω’°’µ’ø’•÷Ä’´ ’¥’∏’∂’´’ø’∏÷Ä’´’∂’£ ’Ω’Ø’Ω’æ’•’¨ ’ß")
    print(f"üìÖ ’Ä’∏’§’æ’°’Æ’∂’•÷Ä’´ ’∫’°’∞’∫’°’∂’¥’°’∂ ’™’°’¥’Ø’•’ø’®’ù {days_to_keep} ÷Ö÷Ä")
    print(f"‚è∞ ’ç’ø’∏÷Ç’£’¥’°’∂ ’¥’´’ª’°’Ø’°’µ÷Ñ’®’ù {interval_minutes} ÷Ä’∏’∫’•")

    # Initialize API client
    api_client = NewsMonitorAPI(api_base_url)
    
    # Test API connection
    api_connected = api_client.test_connection()
    if api_connected:
        try:
            keywords = api_client.get_keywords()
            print(f"‚úÖ API ’Ø’°’∫ ’∞’°’Ω’ø’°’ø’æ’°’Æ, ’¢’°’∂’°’¨’´ ’¢’°’º’•÷Ä’ù {len(keywords)}")
            print(f"üîç Keywords: {[kw.get('word', '') for kw in keywords]}")
        except Exception as e:
            print(f"‚ö†Ô∏è API keywords ’Ω’≠’°’¨’ù {e}")
            keywords = []
    else:
        print("‚ö†Ô∏è API ’Ø’°’∫’´ ’≠’∂’§’´÷Ä, ’∑’°÷Ä’∏÷Ç’∂’°’Ø’∏÷Ç’¥ ’•’∂÷Ñ Scrapy-’∏’æ...")
        keywords = []
    
    # Fallback keywords if API is not available
    if not keywords:
        print("üìù ’ï’£’ø’°’£’∏÷Ä’Æ’∏÷Ç’¥ ’•’∂÷Ñ fallback ’¢’°’∂’°’¨’´ ’¢’°’º’•÷Ä...")
        keywords = [
            {"word": "’Ä’°’µ’°’Ω’ø’°’∂", "is_active": True},
            {"word": "‘µ÷Ä÷á’°’∂", "is_active": True},
            {"word": "’Ü’´’Ø’∏’¨ ’ì’°’∑’´’∂’µ’°’∂", "is_active": True},
            {"word": "‘ø’°’º’°’æ’°÷Ä’∏÷Ç’©’µ’∏÷Ç’∂", "is_active": True},
            {"word": "’ä’°’ø’£’°’¥’°’æ’∏÷Ä", "is_active": True},
            {"word": "‘≤’°’∂’°’Ø", "is_active": True},
            {"word": "’ç’°’∞’¥’°’∂", "is_active": True},
            {"word": "’è’∂’ø’•’Ω’∏÷Ç’©’µ’∏÷Ç’∂", "is_active": True},
            {"word": "‘ø÷Ä’©’∏÷Ç’©’µ’∏÷Ç’∂", "is_active": True},
            {"word": "‘±’º’∏’≤’ª’°’∫’°’∞’∏÷Ç’©’µ’∏÷Ç’∂", "is_active": True}
        ]
        print(f"‚úÖ Fallback ’¢’°’∂’°’¨’´ ’¢’°’º’•÷Ä’ù {len(keywords)}")

    # Set up Scrapy environment for GROUP 1
    scrapy_project_path = os.path.join(os.path.dirname(__file__), 'news_scraper_group1')
    
    # Debug project structure
    check_project_structure(scrapy_project_path)
    
    # Get available spiders with improved method
    spiders = get_spiders_list(scrapy_project_path)
    
    if not spiders:
        print("‚ùå ‘Ω’à’í’Ñ‘≤ 1 - ’ç’°÷Ä’§’•÷Ä ’π’•’∂ ’£’ø’∂’æ’•’¨÷â")
        
        # Try alternative paths
        alternative_paths = [
            os.path.dirname(__file__),  # Current directory
            os.path.join(os.path.dirname(__file__), '..', 'news_scraper_group1'),  # Parent directory
            '/opt/render/project/src/news_scraper_group1',  # Render.com typical path
            '/app/news_scraper_group1',  # Heroku typical path
        ]
        
        print("üîç ’ì’∏÷Ä’±’∏÷Ç’¥ ’•’∂÷Ñ ’°’µ’¨’®’∂’ø÷Ä’°’∂÷Ñ’°’µ’´’∂ ’∏÷Ç’≤’´’∂’•÷Ä...")
        for alt_path in alternative_paths:
            print(f"üìÅ ’ç’ø’∏÷Ç’£’∏÷Ç’¥ ’•’∂÷Ñ’ù {alt_path}")
            if os.path.exists(alt_path):
                print(f"‚úÖ ‘≥’ø’∂’æ’°’Æ’ù {alt_path}")
                check_project_structure(alt_path)
                spiders = get_spiders_list(alt_path)
                if spiders:
                    scrapy_project_path = alt_path
                    break
        
        if not spiders:
            print("‚ùå ‘Ω’à’í’Ñ‘≤ 1 - ‘≤’∏’¨’∏÷Ä ’∏÷Ç’≤’´’∂’•÷Ä’∏÷Ç’¥ ’Ω’°÷Ä’§’•÷Ä ’π’•’∂ ’£’ø’∂’æ’•’¨÷â ‘µ’¨÷Ñ÷â")
            return
    
    print(f"‚úÖ ‘Ω’à’í’Ñ‘≤ 1 - ‘≥’ø’∂’æ’°’Æ ’Ω’°÷Ä’§’•÷Ä’ù {', '.join(spiders)}")

    cycle_count = 0
    
    try:
        while True:
            cycle_count += 1
            print(f"\nüîÑ ‘Ω’à’í’Ñ‘≤ 1 - ’ë’´’Ø’¨ #{cycle_count} - {datetime.now().strftime('%H:%M:%S')}")
            
            # Cleanup old articles via API (only if API is connected)
            if api_connected:
                deleted_count = api_client.cleanup_old_articles(days_to_keep)
                if deleted_count > 0:
                    print(f"üóëÔ∏è ‘Ω’à’í’Ñ‘≤ 1 - API-’´ ’¥’´’ª’∏÷Å’∏’æ ’¥’°÷Ñ÷Ä’æ’•’¨ ’ß {deleted_count} ’∞’´’∂ ’∞’∏’§’æ’°’Æ")
                else:
                    print("‚ö†Ô∏è API cleanup ’¢’°÷Å ’©’∏’≤’∂’æ’°’Æ (’Ø’°’∫ ’π’Ø’°)")

            # Run each spider with reactor fix
            for spider_name in spiders:
                print(f"üï∑Ô∏è ‘Ω’à’í’Ñ‘≤ 1 - ’ç’Ø’Ω’æ’∏÷Ç’¥ ’ß ’Ω’°÷Ä’§’®’ù {spider_name}")
                print(f"üîç Debug: Spider {spider_name} start time: {datetime.now().strftime('%H:%M:%S')}")
                
                try:
                    result = run_scrapy_with_reactor_fix(spider_name, scrapy_project_path)
                    
                    if result.returncode == 0:
                        # Extract key info from output
                        lines = result.stdout.split('\n')
                        found_output = False
                        for line in lines:
                            if any(keyword in line for keyword in ['üìä ‘±’Ñ’ì’à’ì’à’í’Ñ', '‚úÖ ‘≤’°’∂’°’¨’´ ’¢’°’º ’£’ø’∂’æ’•÷Å', 'üíæ ’Ü’∏÷Ä ’∞’∏’§’æ’°’Æ', 'üîÑ ‘ø÷Ä’Ø’∂’∏÷Ç’©’µ’∏÷Ç’∂', 'üìÑ ’Ä’∏’§’æ’°’Æ', 'üîç ‘≥’ø’∂’æ’°’Æ', 'üì∞ ‘≥’ø’∂’æ’•’¨ ’ß', '‚úÖ ‘≤’°’∂’°’¨’´ ’¢’°’º ’£’ø’∂’æ’•÷Å', '‚ùå ‘≤’°’∂’°’¨’´ ’¢’°’º ’π’£’ø’∂’æ’•÷Å']):
                                print(f"    ‘Ω’à’í’Ñ‘≤ 1 - {line.strip()}")
                                found_output = True
                        
                        if not found_output:
                            print(f"    ‘Ω’à’í’Ñ‘≤ 1 - {spider_name}: ’à’π ’¥’´ ’∞’∏’§’æ’°’Æ ’π’´ ’£’ø’∂’æ’•’¨")
                            # Show first few lines of stdout for debugging
                            if result.stdout:
                                print(f"    ‘Ω’à’í’Ñ‘≤ 1 - {spider_name} stdout preview: {result.stdout[:300]}...")
                        
                        # Show stderr if there are any errors
                        if result.stderr:
                            print(f"    ‘Ω’à’í’Ñ‘≤ 1 - {spider_name} stderr: {result.stderr[:200]}...")
                        
                        print(f"‚úÖ ‘Ω’à’í’Ñ‘≤ 1 - {spider_name} ’°’æ’°÷Ä’ø’æ’°’Æ")
                    else:
                        # Print full error details
                        print(f"‚ùå ‘Ω’à’í’Ñ‘≤ 1 - {spider_name} ’Ω’≠’°’¨ (return code: {result.returncode})")
                        if result.stdout:
                            print(f"üìÑ STDOUT: {result.stdout}")
                        if result.stderr:
                            print(f"‚ùå STDERR: {result.stderr}")
                        
                        # If it's a critical error, skip this spider for this cycle
                        error_msg = result.stderr if result.stderr else "Unknown error"
                        if "Could not find spider class" in error_msg or "ImportError" in error_msg:
                            print(f"‚ö†Ô∏è ‘Ω’à’í’Ñ‘≤ 1 - {spider_name} ’¢’°÷Å ’©’∏’≤’∂’æ’°’Æ ’°’µ’Ω ÷Å’´’Ø’¨’∏÷Ç’¥")
                        
                except subprocess.TimeoutExpired:
                    print(f"‚è∞ ‘Ω’à’í’Ñ‘≤ 1 - {spider_name} timeout (2 ÷Ä’∏’∫’•)")
                    print(f"üîç Debug: Spider {spider_name} took too long, skipping...")
                except Exception as e:
                    print(f"‚ùå ‘Ω’à’í’Ñ‘≤ 1 - {spider_name} ’Ω’≠’°’¨: {e}")
                    # Continue with next spider instead of crashing
                    continue

            print(f"‚úÖ ‘Ω’à’í’Ñ‘≤ 1 - ’ë’´’Ø’¨ #{cycle_count} ’°’æ’°÷Ä’ø’æ’°’Æ")
            print(f"üò¥ ‘Ω’à’í’Ñ‘≤ 1 - ’Ä’°’ª’∏÷Ä’§ ’Ω’ø’∏÷Ç’£’∏÷Ç’¥’®’ù {interval_minutes} ÷Ä’∏’∫’•’´÷Å...")
            
            # Sleep for the specified interval
            time.sleep(interval_minutes * 60)
            
    except KeyboardInterrupt:
        print("\nüõë ‘Ω’à’í’Ñ‘≤ 1 - ’Ñ’∏’∂’´’ø’∏÷Ä’´’∂’£’® ’§’°’§’°÷Ä’•÷Å’æ’•’¨ ’ß ÷Ö’£’ø’°’£’∏÷Ä’Æ’∏’≤’´ ’Ø’∏’≤’¥’´÷Å")
    except Exception as e:
        print(f"‚ùå ‘Ω’à’í’Ñ‘≤ 1 - ‘∏’∂’§’∞’°’∂’∏÷Ç÷Ä ’Ω’≠’°’¨: {e}")

if __name__ == "__main__":
    main()