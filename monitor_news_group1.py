print("ğŸ” Debug: Starting imports...")
import time
print("ğŸ” Debug: time imported")
import os
print("ğŸ” Debug: os imported")
import sys
print("ğŸ” Debug: sys imported")
import subprocess
print("ğŸ” Debug: subprocess imported")
import requests
print("ğŸ” Debug: requests imported")
import json
print("ğŸ” Debug: json imported")
from datetime import datetime, timedelta
print("ğŸ” Debug: datetime imported")
print("ğŸ” Debug: All imports completed")

# Memory optimization imports
import gc
import psutil

def cleanup_memory():
    """Memory cleanup function"""
    try:
        # Force garbage collection
        gc.collect()
        
        # Get memory info
        process = psutil.Process()
        memory_info = process.memory_info()
        memory_mb = memory_info.rss / 1024 / 1024
        
        print(f"ğŸ§¹ Memory cleanup: {memory_mb:.1f} MB")
        return memory_mb
    except Exception as e:
        print(f"âš ï¸ Memory cleanup error: {e}")
        return 0

class NewsMonitorAPI:
    def __init__(self, api_base_url):
        self.api_base_url = api_base_url.rstrip('/')
        self.session = requests.Session()
        self.session.headers.update({
            'Content-Type': 'application/json',
            'User-Agent': 'NewsMonitor/1.0'
        })
        
    def test_connection(self):
        """API-Õ« Õ¯Õ¡ÕºÕ« Õ½Õ¿Õ¸Ö‚Õ£Õ¸Ö‚Õ´"""
        try:
            print(f"ğŸ”— ÕÕ¿Õ¸Ö‚Õ£Õ¸Ö‚Õ´ Õ¥Õ¶Ö„ API Õ¯Õ¡ÕºÕ¨Õ {self.api_base_url}")
            
            # Õ“Õ¸Ö€Õ±Õ¸Ö‚Õ´ Õ¥Õ¶Ö„ Õ°Õ«Õ´Õ¶Õ¡Õ¯Õ¡Õ¶ endpoint-Õ¶Õ¥Ö€Õ¨
            test_endpoints = [
                f"{self.api_base_url}/",
                f"{self.api_base_url}/api/",
                f"{self.api_base_url}/health/",
                f"{self.api_base_url}/status/"
            ]
            
            for endpoint in test_endpoints:
                try:
                    response = self.session.get(endpoint, timeout=5)
                    if response.status_code in [200, 404]:  # 404-Õ¨ Õ¶Õ¸Ö‚ÕµÕ¶ÕºÕ¥Õ½ OK Õ§, Õ¶Õ·Õ¡Õ¶Õ¡Õ¯Õ¸Ö‚Õ´ Õ§ endpoint Õ£Õ¸ÕµÕ¸Ö‚Õ©ÕµÕ¸Ö‚Õ¶ Õ¸Ö‚Õ¶Õ«
                        print(f"âœ… API Õ¯Õ¡Õº Õ°Õ¡Õ»Õ¸Õ²Õ {endpoint}")
                        return True
                except:
                    continue
            
            print(f"âš ï¸ API Õ¯Õ¡ÕºÕ« Õ­Õ¶Õ¤Õ«Ö€Õ {self.api_base_url}")
            return False
            
        except Exception as e:
            print(f"âŒ API Õ¯Õ¡ÕºÕ« Õ½Õ­Õ¡Õ¬Õ {e}")
            return False
    
    def cleanup_old_articles(self, days_to_keep):
        """Õ€Õ«Õ¶ Õ°Õ¸Õ¤Õ¾Õ¡Õ®Õ¶Õ¥Ö€Õ« Õ´Õ¡Ö„Ö€Õ¸Ö‚Õ´ API-Õ« Õ´Õ«Õ»Õ¸ÖÕ¸Õ¾"""
        try:
            cleanup_date = (datetime.now() - timedelta(days=days_to_keep)).isoformat()
            print(f"ğŸ§¹ Õ“Õ¸Ö€Õ±Õ¸Ö‚Õ´ Õ¥Õ¶Ö„ Õ´Õ¡Ö„Ö€Õ¥Õ¬ Õ°Õ¸Õ¤Õ¾Õ¡Õ®Õ¶Õ¥Ö€Õ¨ {cleanup_date} Õ¡Õ´Õ½Õ¡Õ©Õ¾Õ«Ö Õ¡Õ¼Õ¡Õ»...")
            
            # Õ“Õ¸Ö€Õ±Õ¸Ö‚Õ´ Õ¥Õ¶Ö„ Õ¿Õ¡Ö€Õ¢Õ¥Ö€ endpoint-Õ¶Õ¥Ö€
            endpoints_to_try = [
                f"{self.api_base_url}/api/articles/cleanup/",
                f"{self.api_base_url}/api/cleanup/",
                f"{self.api_base_url}/api/articles/",
                f"{self.api_base_url}/cleanup/"
            ]
            
            for endpoint in endpoints_to_try:
                try:
                    print(f"ğŸ”— Õ“Õ¸Ö€Õ±Õ¸Ö‚Õ´ Õ¥Õ¶Ö„ endpointÕ {endpoint}")
                    response = self.session.delete(
                        endpoint,
                        params={'before_date': cleanup_date},
                        timeout=10
                    )
                    
                    if response.status_code == 200:
                        data = response.json()
                        deleted_count = data.get('deleted_count', 0)
                        print(f"âœ… Õ€Õ¡Õ»Õ¸Õ²Õ¸Ö‚Õ©ÕµÕ¡Õ´Õ¢ Õ´Õ¡Ö„Ö€Õ¾Õ¥Ö {deleted_count} Õ°Õ¸Õ¤Õ¾Õ¡Õ®")
                        return deleted_count
                    elif response.status_code == 404:
                        print(f"âš ï¸ Endpoint Õ¹Õ« Õ£Õ¿Õ¶Õ¾Õ¥Õ¬Õ {endpoint}")
                        continue
                    else:
                        print(f"âŒ API cleanup error: {response.status_code} - {response.text}")
                        continue
                        
                except requests.exceptions.RequestException as e:
                    print(f"âš ï¸ Network error {endpoint}: {e}")
                    continue
            
            print("âŒ ÕˆÕ¹ Õ´Õ« endpoint Õ¹Õ¡Õ·Õ­Õ¡Õ¿Õ¥Ö")
            return 0
            
        except Exception as e:
            print(f"âŒ API cleanup exception: {e}")
            return 0
    
    def get_keywords(self):
        """Ô²Õ¡Õ¶Õ¡Õ¬Õ« Õ¢Õ¡Õ¼Õ¥Ö€Õ« Õ½Õ¿Õ¡ÖÕ¸Ö‚Õ´ API-Õ«Ö"""
        try:
            print(f"ğŸ” Õ“Õ¸Ö€Õ±Õ¸Ö‚Õ´ Õ¥Õ¶Ö„ Õ½Õ¿Õ¡Õ¶Õ¡Õ¬ Õ¢Õ¡Õ¶Õ¡Õ¬Õ« Õ¢Õ¡Õ¼Õ¥Ö€...")
            
            # Õ“Õ¸Ö€Õ±Õ¸Ö‚Õ´ Õ¥Õ¶Ö„ Õ¿Õ¡Ö€Õ¢Õ¥Ö€ endpoint-Õ¶Õ¥Ö€
            endpoints_to_try = [
                f"{self.api_base_url}/api/keywords/",
                f"{self.api_base_url}/api/keywords",
                f"{self.api_base_url}/keywords/",
                f"{self.api_base_url}/keywords"
            ]
            
            for endpoint in endpoints_to_try:
                try:
                    print(f"ğŸ”— Õ“Õ¸Ö€Õ±Õ¸Ö‚Õ´ Õ¥Õ¶Ö„ endpointÕ {endpoint}")
                    response = self.session.get(endpoint, timeout=10)
                    
                    if response.status_code == 200:
                        keywords = response.json()
                        print(f"âœ… ÕÕ¿Õ¡ÖÕ¾Õ¥Ö {len(keywords)} Õ¢Õ¡Õ¶Õ¡Õ¬Õ« Õ¢Õ¡Õ¼")
                        return keywords
                    elif response.status_code == 404:
                        print(f"âš ï¸ Endpoint Õ¹Õ« Õ£Õ¿Õ¶Õ¾Õ¥Õ¬Õ {endpoint}")
                        continue
                    else:
                        print(f"âŒ API keywords error: {response.status_code} - {response.text}")
                        continue
                        
                except requests.exceptions.RequestException as e:
                    print(f"âš ï¸ Network error {endpoint}: {e}")
                    continue
            
            print("âŒ ÕˆÕ¹ Õ´Õ« keywords endpoint Õ¹Õ¡Õ·Õ­Õ¡Õ¿Õ¥Ö")
            return []
            
        except Exception as e:
            print(f"âŒ API keywords exception: {e}")
            return []

def run_scrapy_with_reactor_fix(spider_name, scrapy_project_path):
    """Run scrapy with reactor signal handling fix"""
    try:
        # Set environment variables to fix reactor issues
        env = dict(os.environ)
        env.update({
            'SCRAPY_SETTINGS_MODULE': 'news_scraper.settings',
            'PYTHONPATH': scrapy_project_path,
            # Disable signal handling that causes issues in containerized environments
            'TWISTED_DISABLE_SIGNAL_HANDLERS': '1',
            'TWISTED_NO_SIGNAL_HANDLERS': '1',
            # Suppress all warnings
            'PYTHONWARNINGS': 'ignore'
        })
        
        # Use simple scrapy crawl command with environment variables
        result = subprocess.run([
            sys.executable, '-m', 'scrapy', 'crawl', spider_name
        ], 
            cwd=scrapy_project_path,
            capture_output=True,
            text=True,
            timeout=120,  # 2 minutes per spider
            env=env
        )
        
        # Print full error details for debugging
        if result.returncode != 0:
            print(f"âŒ Spider {spider_name} failed with return code: {result.returncode}")
            print(f"ğŸ“„ STDOUT: {result.stdout}")
            print(f"âŒ STDERR: {result.stderr}")
        
        # Memory cleanup after spider finishes
        memory_usage = cleanup_memory()
        print(f"ğŸ§¹ Spider {spider_name} finished, memory usage: {memory_usage:.1f} MB")
        
        return result
        
    except Exception as reactor_error:
        print(f"âŒ Scrapy crawl failed: {reactor_error}")
        # Return a mock result to prevent crashes
        from types import SimpleNamespace
        mock_result = SimpleNamespace()
        mock_result.returncode = 1
        mock_result.stdout = ""
        mock_result.stderr = f"Scrapy crawl failed: {reactor_error}"
        return mock_result

def get_spiders_list(scrapy_project_path):
    """Get list of available spiders by scanning spider files directly"""
    spiders = []
    
    # Method 1: Direct file scanning (most reliable)
    spiders_dir = os.path.join(scrapy_project_path, 'news_scraper', 'spiders')
    print(f"ğŸ” ÕÕ¿Õ¸Ö‚Õ£Õ¸Ö‚Õ´ Õ¥Õ¶Ö„ spiders ÕºÕ¡Õ¶Õ¡Õ¯Õ¨Õ {spiders_dir}")
    
    if os.path.exists(spiders_dir):
        try:
            for filename in os.listdir(spiders_dir):
                if filename.endswith('.py') and filename != '__init__.py':
                    spider_file = os.path.join(spiders_dir, filename)
                    print(f"ğŸ“„ Ô³Õ¿Õ¶Õ¾Õ¡Õ® spider Ö†Õ¡ÕµÕ¬Õ {filename}")
                    
                    # Try to extract spider name from file
                    try:
                        with open(spider_file, 'r', encoding='utf-8') as f:
                            content = f.read()
                            # Look for name = 'spider_name' pattern
                            import re
                            name_matches = re.findall(r"name\s*=\s*['\"]([^'\"]+)['\"]", content)
                            if name_matches:
                                spider_name = name_matches[0]
                                spiders.append(spider_name)
                                print(f"âœ… Ô³Õ¿Õ¶Õ¾Õ¡Õ® spiderÕ {spider_name}")
                            else:
                                # Fallback: use filename without .py
                                spider_name = filename[:-3]
                                if spider_name not in ['__init__', 'base']:
                                    spiders.append(spider_name)
                                    print(f"âœ… Fallback spiderÕ {spider_name}")
                    except Exception as e:
                        print(f"âš ï¸ Õ‰Õ°Õ¡Õ»Õ¸Õ²Õ¾Õ¥Ö Õ¯Õ¡Ö€Õ¤Õ¡Õ¬ {filename}: {e}")
                        
        except Exception as e:
            print(f"âŒ Spiders ÕºÕ¡Õ¶Õ¡Õ¯Õ« Õ½Õ¯Õ¡Õ¶Õ¡Õ¾Õ¸Ö€Õ´Õ¡Õ¶ Õ½Õ­Õ¡Õ¬: {e}")
    else:
        print(f"âŒ Spiders ÕºÕ¡Õ¶Õ¡Õ¯Õ¨ Õ£Õ¸ÕµÕ¸Ö‚Õ©ÕµÕ¸Ö‚Õ¶ Õ¹Õ¸Ö‚Õ¶Õ«Õ {spiders_dir}")
    
    # Method 2: Try Scrapy's spider loader as fallback (only if Method 1 failed)
    if not spiders:
        print("ğŸ”„ Õ“Õ¸Ö€Õ±Õ¸Ö‚Õ´ Õ¥Õ¶Ö„ Scrapy-Õ« spider loader...")
        try:
            env = dict(os.environ)
            env.update({
                'SCRAPY_SETTINGS_MODULE': 'news_scraper.settings',
                'PYTHONPATH': scrapy_project_path,
                # Use Python 3.11 for better compatibility
                'PYTHON_VERSION': '3.11',
                # Use stable Twisted version
                'TWISTED_VERSION': '22.10.0',
                # Force asyncio reactor before any imports
                'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
                'TWISTED_DISABLE_SIGNAL_HANDLERS': '1',
                'TWISTED_NO_SIGNAL_HANDLERS': '1',
                'PYTHONWARNINGS': 'ignore'
            })
            
            # Simpler approach without using deprecated modules
            result = subprocess.run([
                sys.executable, '-W', 'ignore', '-c', f'''
import warnings
warnings.filterwarnings("ignore")

import sys
import os
sys.path.insert(0, "{scrapy_project_path}")

try:
    import importlib.util
    import glob
    
    spider_files = glob.glob("{spiders_dir}/*.py")
    for spider_file in spider_files:
        if not spider_file.endswith("__init__.py"):
            with open(spider_file, 'r', encoding='utf-8') as f:
                content = f.read()
                if 'class' in content and 'Spider' in content:
                    import re
                    names = re.findall(r"name\\s*=\\s*['\\""]([^'\\"\"]+)['\\""]", content)
                    if names:
                        print(names[0])
                    else:
                        basename = os.path.basename(spider_file)[:-3]
                        if basename not in ['__init__', 'base']:
                            print(basename)
except Exception as subprocess_error:
    print(f"Error: {{subprocess_error}}", file=sys.stderr)
    sys.exit(1)
'''],
                cwd=scrapy_project_path,
                capture_output=True,
                text=True,
                timeout=30,
                env=env
            )
            
            if result.returncode == 0 and result.stdout.strip():
                fallback_spiders = [spider.strip() for spider in result.stdout.strip().split('\n') if spider.strip()]
                spiders.extend(fallback_spiders)
                print(f"âœ… Scrapy loader Õ¡Ö€Õ¤ÕµÕ¸Ö‚Õ¶Ö„Õ {fallback_spiders}")
            else:
                print(f"âŒ Scrapy loader Õ±Õ¡Õ­Õ¸Õ²Õ¸Ö‚Õ´: {result.stderr}")
                
        except Exception as e:
            print(f"âŒ Scrapy loader Õ½Õ­Õ¡Õ¬: {e}")
    
    # Method 3: Hardcoded fallback for common spider names
    if not spiders:
        print("ğŸ”„ Õ“Õ¸Ö€Õ±Õ¸Ö‚Õ´ Õ¥Õ¶Ö„ Õ½Õ¿Õ¡Õ¶Õ¤Õ¡Ö€Õ¿ spider Õ¡Õ¶Õ¸Ö‚Õ¶Õ¶Õ¥Ö€...")
        common_spiders = ['asbarez', 'aravot', 'news_am', 'panorama', 'hetq']
        for spider_name in common_spiders:
            spider_file = os.path.join(spiders_dir, f'{spider_name}.py')
            if os.path.exists(spider_file):
                spiders.append(spider_name)
                print(f"âœ… ÕÕ¿Õ¡Õ¶Õ¤Õ¡Ö€Õ¿ spider Õ£Õ¿Õ¶Õ¾Õ¡Õ®Õ {spider_name}")
    
    return list(set(spiders))  # Remove duplicates

def check_project_structure(scrapy_project_path):
    """Check and debug project structure"""
    print(f"ğŸ” ÕÕ¿Õ¸Ö‚Õ£Õ¸Ö‚Õ´ Õ¥Õ¶Ö„ Õ¶Õ¡Õ­Õ¡Õ£Õ®Õ« Õ¯Õ¡Õ¼Õ¸Ö‚ÖÕ¾Õ¡Õ®Ö„Õ¨...")
    print(f"ğŸ“ Project path: {scrapy_project_path}")
    print(f"ğŸ“ Exists: {os.path.exists(scrapy_project_path)}")
    
    if os.path.exists(scrapy_project_path):
        print("ğŸ“‚ Project directory contents:")
        try:
            for item in os.listdir(scrapy_project_path):
                item_path = os.path.join(scrapy_project_path, item)
                if os.path.isdir(item_path):
                    print(f"  ğŸ“ {item}/")
                else:
                    print(f"  ğŸ“„ {item}")
        except Exception as e:
            print(f"âŒ Could not list project directory: {e}")
    
    # Check news_scraper directory
    news_scraper_path = os.path.join(scrapy_project_path, 'news_scraper')
    print(f"ğŸ“ news_scraper path: {news_scraper_path}")
    print(f"ğŸ“ Exists: {os.path.exists(news_scraper_path)}")
    
    if os.path.exists(news_scraper_path):
        print("ğŸ“‚ news_scraper directory contents:")
        try:
            for item in os.listdir(news_scraper_path):
                item_path = os.path.join(news_scraper_path, item)
                if os.path.isdir(item_path):
                    print(f"  ğŸ“ {item}/")
                else:
                    print(f"  ğŸ“„ {item}")
        except Exception as e:
            print(f"âŒ Could not list news_scraper directory: {e}")
    
    # Check spiders directory
    spiders_path = os.path.join(scrapy_project_path, 'news_scraper', 'spiders')
    print(f"ğŸ“ spiders path: {spiders_path}")
    print(f"ğŸ“ Exists: {os.path.exists(spiders_path)}")
    
    if os.path.exists(spiders_path):
        print("ğŸ“‚ spiders directory contents:")
        try:
            for item in os.listdir(spiders_path):
                item_path = os.path.join(spiders_path, item)
                if os.path.isdir(item_path):
                    print(f"  ğŸ“ {item}/")
                else:
                    print(f"  ğŸ“„ {item}")
        except Exception as e:
            print(f"âŒ Could not list spiders directory: {e}")

def main():
    print("ğŸ¢ Ô½ÕˆÕ’Õ„Ô² 1 - Õ„Õ¥Õ® Õ¶ÕµÕ¸Ö‚Õ¦ Õ½Õ¡ÕµÕ¿Õ¥Ö€Õ« Õ´Õ¸Õ¶Õ«Õ¿Õ¸Ö€Õ«Õ¶Õ£ (news_scraper_group1)")
    print("ğŸ” Debug: main() function started")
    
    # Get settings from environment variables or use defaults
    interval_minutes = int(os.environ.get('MONITOR_INTERVAL_MINUTES', 2))
    days_to_keep = int(os.environ.get('DAYS_TO_KEEP_ARTICLES', 7))
    api_base_url = os.environ.get('API_BASE_URL', 'https://beackkayq.onrender.com')
    
    print(f"ğŸŒ API Base URL: {api_base_url}")
    print(f"ğŸ” Debug: interval_minutes = {interval_minutes}")
    print(f"ğŸ” Debug: days_to_keep = {days_to_keep}")
    
    print("ğŸš€ Ô½ÕˆÕ’Õ„Ô² 1 - Õ„Õ¥Õ® Õ¶ÕµÕ¸Ö‚Õ¦ Õ½Õ¡ÕµÕ¿Õ¥Ö€Õ« Õ´Õ¸Õ¶Õ«Õ¿Õ¸Ö€Õ«Õ¶Õ£ Õ½Õ¯Õ½Õ¾Õ¥Õ¬ Õ§")
    print(f"ğŸ“… Õ€Õ¸Õ¤Õ¾Õ¡Õ®Õ¶Õ¥Ö€Õ« ÕºÕ¡Õ°ÕºÕ¡Õ¶Õ´Õ¡Õ¶ ÕªÕ¡Õ´Õ¯Õ¥Õ¿Õ¨Õ {days_to_keep} Ö…Ö€")
    print(f"â° ÕÕ¿Õ¸Ö‚Õ£Õ´Õ¡Õ¶ Õ´Õ«Õ»Õ¡Õ¯Õ¡ÕµÖ„Õ¨Õ {interval_minutes} Ö€Õ¸ÕºÕ¥")

    # Initialize API client
    api_client = NewsMonitorAPI(api_base_url)
    
    # Test API connection
    api_connected = api_client.test_connection()
    if api_connected:
        try:
            keywords = api_client.get_keywords()
            print(f"âœ… API Õ¯Õ¡Õº Õ°Õ¡Õ½Õ¿Õ¡Õ¿Õ¾Õ¡Õ®, Õ¢Õ¡Õ¶Õ¡Õ¬Õ« Õ¢Õ¡Õ¼Õ¥Ö€Õ {len(keywords)}")
            print(f"ğŸ” Keywords: {[kw.get('word', '') for kw in keywords]}")
        except Exception as e:
            print(f"âš ï¸ API keywords Õ½Õ­Õ¡Õ¬Õ {e}")
            keywords = []
    else:
        print("âš ï¸ API Õ¯Õ¡ÕºÕ« Õ­Õ¶Õ¤Õ«Ö€, Õ·Õ¡Ö€Õ¸Ö‚Õ¶Õ¡Õ¯Õ¸Ö‚Õ´ Õ¥Õ¶Ö„ Scrapy-Õ¸Õ¾...")
        keywords = []
    
    # Fallback keywords if API is not available
    if not keywords:
        print("ğŸ“ Õ•Õ£Õ¿Õ¡Õ£Õ¸Ö€Õ®Õ¸Ö‚Õ´ Õ¥Õ¶Ö„ fallback Õ¢Õ¡Õ¶Õ¡Õ¬Õ« Õ¢Õ¡Õ¼Õ¥Ö€...")
        keywords = [
            {"word": "Õ€Õ¡ÕµÕ¡Õ½Õ¿Õ¡Õ¶", "is_active": True},
            {"word": "ÔµÖ€Ö‡Õ¡Õ¶", "is_active": True},
            {"word": "Õ†Õ«Õ¯Õ¸Õ¬ Õ“Õ¡Õ·Õ«Õ¶ÕµÕ¡Õ¶", "is_active": True},
            {"word": "Ô¿Õ¡Õ¼Õ¡Õ¾Õ¡Ö€Õ¸Ö‚Õ©ÕµÕ¸Ö‚Õ¶", "is_active": True},
            {"word": "ÕŠÕ¡Õ¿Õ£Õ¡Õ´Õ¡Õ¾Õ¸Ö€", "is_active": True},
            {"word": "Ô²Õ¡Õ¶Õ¡Õ¯", "is_active": True},
            {"word": "ÕÕ¡Õ°Õ´Õ¡Õ¶", "is_active": True},
            {"word": "ÕÕ¶Õ¿Õ¥Õ½Õ¸Ö‚Õ©ÕµÕ¸Ö‚Õ¶", "is_active": True},
            {"word": "Ô¿Ö€Õ©Õ¸Ö‚Õ©ÕµÕ¸Ö‚Õ¶", "is_active": True},
            {"word": "Ô±Õ¼Õ¸Õ²Õ»Õ¡ÕºÕ¡Õ°Õ¸Ö‚Õ©ÕµÕ¸Ö‚Õ¶", "is_active": True}
        ]
        print(f"âœ… Fallback Õ¢Õ¡Õ¶Õ¡Õ¬Õ« Õ¢Õ¡Õ¼Õ¥Ö€Õ {len(keywords)}")

    # Set up Scrapy environment for GROUP 1
    scrapy_project_path = os.path.join(os.path.dirname(__file__), 'news_scraper_group1')
    
    # Debug project structure
    check_project_structure(scrapy_project_path)
    
    # Get available spiders with improved method
    spiders = get_spiders_list(scrapy_project_path)
    
    if not spiders:
        print("âŒ Ô½ÕˆÕ’Õ„Ô² 1 - ÕÕ¡Ö€Õ¤Õ¥Ö€ Õ¹Õ¥Õ¶ Õ£Õ¿Õ¶Õ¾Õ¥Õ¬Ö‰")
        
        # Try alternative paths
        alternative_paths = [
            os.path.dirname(__file__),  # Current directory
            os.path.join(os.path.dirname(__file__), '..', 'news_scraper_group1'),  # Parent directory
            '/opt/render/project/src/news_scraper_group1',  # Render.com typical path
            '/app/news_scraper_group1',  # Heroku typical path
        ]
        
        print("ğŸ” Õ“Õ¸Ö€Õ±Õ¸Ö‚Õ´ Õ¥Õ¶Ö„ Õ¡ÕµÕ¬Õ¨Õ¶Õ¿Ö€Õ¡Õ¶Ö„Õ¡ÕµÕ«Õ¶ Õ¸Ö‚Õ²Õ«Õ¶Õ¥Ö€...")
        for alt_path in alternative_paths:
            print(f"ğŸ“ ÕÕ¿Õ¸Ö‚Õ£Õ¸Ö‚Õ´ Õ¥Õ¶Ö„Õ {alt_path}")
            if os.path.exists(alt_path):
                print(f"âœ… Ô³Õ¿Õ¶Õ¾Õ¡Õ®Õ {alt_path}")
                check_project_structure(alt_path)
                spiders = get_spiders_list(alt_path)
                if spiders:
                    scrapy_project_path = alt_path
                    break
        
        if not spiders:
            print("âŒ Ô½ÕˆÕ’Õ„Ô² 1 - Ô²Õ¸Õ¬Õ¸Ö€ Õ¸Ö‚Õ²Õ«Õ¶Õ¥Ö€Õ¸Ö‚Õ´ Õ½Õ¡Ö€Õ¤Õ¥Ö€ Õ¹Õ¥Õ¶ Õ£Õ¿Õ¶Õ¾Õ¥Õ¬Ö‰ ÔµÕ¬Ö„Ö‰")
            return
    
    print(f"âœ… Ô½ÕˆÕ’Õ„Ô² 1 - Ô³Õ¿Õ¶Õ¾Õ¡Õ® Õ½Õ¡Ö€Õ¤Õ¥Ö€Õ {', '.join(spiders)}")

    cycle_count = 0
    
    try:
        while True:
            cycle_count += 1
            print(f"\nğŸ”„ Ô½ÕˆÕ’Õ„Ô² 1 - Õ‘Õ«Õ¯Õ¬ #{cycle_count} - {datetime.now().strftime('%H:%M:%S')}")
            
            # Cleanup old articles via API (only if API is connected)
            if api_connected:
                deleted_count = api_client.cleanup_old_articles(days_to_keep)
                if deleted_count > 0:
                    print(f"ğŸ—‘ï¸ Ô½ÕˆÕ’Õ„Ô² 1 - API-Õ« Õ´Õ«Õ»Õ¸ÖÕ¸Õ¾ Õ´Õ¡Ö„Ö€Õ¾Õ¥Õ¬ Õ§ {deleted_count} Õ°Õ«Õ¶ Õ°Õ¸Õ¤Õ¾Õ¡Õ®")
                else:
                    print("âš ï¸ API cleanup Õ¢Õ¡Ö Õ©Õ¸Õ²Õ¶Õ¾Õ¡Õ® (Õ¯Õ¡Õº Õ¹Õ¯Õ¡)")

            # Run each spider with reactor fix
            for spider_name in spiders:
                print(f"ğŸ•·ï¸ Ô½ÕˆÕ’Õ„Ô² 1 - ÕÕ¯Õ½Õ¾Õ¸Ö‚Õ´ Õ§ Õ½Õ¡Ö€Õ¤Õ¨Õ {spider_name}")
                print(f"ğŸ” Debug: Spider {spider_name} start time: {datetime.now().strftime('%H:%M:%S')}")
                
                try:
                    result = run_scrapy_with_reactor_fix(spider_name, scrapy_project_path)
                    
                    if result.returncode == 0:
                        # Extract key info from output
                        lines = result.stdout.split('\n')
                        found_output = False
                        for line in lines:
                            if any(keyword in line for keyword in ['ğŸ“Š Ô±Õ„Õ“ÕˆÕ“ÕˆÕ’Õ„', 'âœ… Ô²Õ¡Õ¶Õ¡Õ¬Õ« Õ¢Õ¡Õ¼ Õ£Õ¿Õ¶Õ¾Õ¥Ö', 'ğŸ’¾ Õ†Õ¸Ö€ Õ°Õ¸Õ¤Õ¾Õ¡Õ®', 'ğŸ”„ Ô¿Ö€Õ¯Õ¶Õ¸Ö‚Õ©ÕµÕ¸Ö‚Õ¶', 'ğŸ“„ Õ€Õ¸Õ¤Õ¾Õ¡Õ®', 'ğŸ” Ô³Õ¿Õ¶Õ¾Õ¡Õ®', 'ğŸ“° Ô³Õ¿Õ¶Õ¾Õ¥Õ¬ Õ§', 'âœ… Ô²Õ¡Õ¶Õ¡Õ¬Õ« Õ¢Õ¡Õ¼ Õ£Õ¿Õ¶Õ¾Õ¥Ö', 'âŒ Ô²Õ¡Õ¶Õ¡Õ¬Õ« Õ¢Õ¡Õ¼ Õ¹Õ£Õ¿Õ¶Õ¾Õ¥Ö']):
                                print(f"    Ô½ÕˆÕ’Õ„Ô² 1 - {line.strip()}")
                                found_output = True
                        
                        if not found_output:
                            print(f"    Ô½ÕˆÕ’Õ„Ô² 1 - {spider_name}: ÕˆÕ¹ Õ´Õ« Õ°Õ¸Õ¤Õ¾Õ¡Õ® Õ¹Õ« Õ£Õ¿Õ¶Õ¾Õ¥Õ¬")
                            # Show first few lines of stdout for debugging
                            if result.stdout:
                                print(f"    Ô½ÕˆÕ’Õ„Ô² 1 - {spider_name} stdout preview: {result.stdout[:300]}...")
                        
                        # Show stderr if there are any errors
                        if result.stderr:
                            print(f"    Ô½ÕˆÕ’Õ„Ô² 1 - {spider_name} stderr: {result.stderr[:200]}...")
                        
                        print(f"âœ… Ô½ÕˆÕ’Õ„Ô² 1 - {spider_name} Õ¡Õ¾Õ¡Ö€Õ¿Õ¾Õ¡Õ®")
                    else:
                        # Print full error details
                        print(f"âŒ Ô½ÕˆÕ’Õ„Ô² 1 - {spider_name} Õ½Õ­Õ¡Õ¬ (return code: {result.returncode})")
                        if result.stdout:
                            print(f"ğŸ“„ STDOUT: {result.stdout}")
                        if result.stderr:
                            print(f"âŒ STDERR: {result.stderr}")
                        
                        # If it's a critical error, skip this spider for this cycle
                        error_msg = result.stderr if result.stderr else "Unknown error"
                        if "Could not find spider class" in error_msg or "ImportError" in error_msg:
                            print(f"âš ï¸ Ô½ÕˆÕ’Õ„Ô² 1 - {spider_name} Õ¢Õ¡Ö Õ©Õ¸Õ²Õ¶Õ¾Õ¡Õ® Õ¡ÕµÕ½ ÖÕ«Õ¯Õ¬Õ¸Ö‚Õ´")
                        
                except subprocess.TimeoutExpired:
                    print(f"â° Ô½ÕˆÕ’Õ„Ô² 1 - {spider_name} timeout (2 Ö€Õ¸ÕºÕ¥)")
                    print(f"ğŸ” Debug: Spider {spider_name} took too long, skipping...")
                except Exception as e:
                    print(f"âŒ Ô½ÕˆÕ’Õ„Ô² 1 - {spider_name} Õ½Õ­Õ¡Õ¬: {e}")
                    # Continue with next spider instead of crashing
                    continue

            print(f"âœ… Ô½ÕˆÕ’Õ„Ô² 1 - Õ‘Õ«Õ¯Õ¬ #{cycle_count} Õ¡Õ¾Õ¡Ö€Õ¿Õ¾Õ¡Õ®")
            print(f"ğŸ˜´ Ô½ÕˆÕ’Õ„Ô² 1 - Õ€Õ¡Õ»Õ¸Ö€Õ¤ Õ½Õ¿Õ¸Ö‚Õ£Õ¸Ö‚Õ´Õ¨Õ {interval_minutes} Ö€Õ¸ÕºÕ¥Õ«Ö...")
            
            # Sleep for the specified interval
            time.sleep(interval_minutes * 60)
            
    except KeyboardInterrupt:
        print("\nğŸ›‘ Ô½ÕˆÕ’Õ„Ô² 1 - Õ„Õ¸Õ¶Õ«Õ¿Õ¸Ö€Õ«Õ¶Õ£Õ¨ Õ¤Õ¡Õ¤Õ¡Ö€Õ¥ÖÕ¾Õ¥Õ¬ Õ§ Ö…Õ£Õ¿Õ¡Õ£Õ¸Ö€Õ®Õ¸Õ²Õ« Õ¯Õ¸Õ²Õ´Õ«Ö")
    except Exception as e:
        print(f"âŒ Ô½ÕˆÕ’Õ„Ô² 1 - Ô¸Õ¶Õ¤Õ°Õ¡Õ¶Õ¸Ö‚Ö€ Õ½Õ­Õ¡Õ¬: {e}")

if __name__ == "__main__":
    main()